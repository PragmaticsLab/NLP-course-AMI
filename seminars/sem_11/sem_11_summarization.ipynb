{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ws1FXPusI0Bh"
   },
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTgTMQb8M0aN"
   },
   "source": [
    "## Deep Reinforced Model for Abstractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qVan4a0tNCJY"
   },
   "source": [
    "[A Deep Reinforced Model for Abstractive Summarization (Romain Paulus, Caiming Xiong, Richard Socher, 2017)](https://arxiv.org/abs/1705.04304) - модель суммаризации на основе encoder-decoder с испльзованием reinforcement learning для обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUtEp0ZlS6xK"
   },
   "source": [
    "### Источники\n",
    "1. Блог с описанием статьи: https://blog.einstein.ai/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization/\n",
    "2. Имплементация: https://github.com/rohithreddy024/Text-Summarizer-Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "--9IWPY_gK-n",
    "outputId": "d445c973-3787-4ded-fcbe-bff22fd08aa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Text-Summarizer-Pytorch'...\n",
      "remote: Enumerating objects: 98, done.\u001b[K\n",
      "remote: Total 98 (delta 0), reused 0 (delta 0), pack-reused 98\u001b[K\n",
      "Unpacking objects: 100% (98/98), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/rohithreddy024/Text-Summarizer-Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VSI7sgTDiks1"
   },
   "source": [
    "### Данные \n",
    "Модель обучается на данных Gigaword dataset: https://github.com/harvardnlp/sent-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TUWvD95NVaTZ"
   },
   "outputs": [],
   "source": [
    "# find the share link of the file/folder on Google Drive\n",
    "file_share_link = \"https://drive.google.com/open?id=0B6N7tANPyVeBNmlSX19Ld2xDU1E\"\n",
    "\n",
    "# extract the ID of the file\n",
    "file_id = file_share_link[file_share_link.find(\"=\") + 1:]\n",
    "\n",
    "# append the id to this REST command\n",
    "file_download_link = \"https://docs.google.com/uc?export=download&id=\" + file_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oe4k_BLYYJot",
    "outputId": "32bb0ec2-64f5-46d4-b58d-ebfc1b80cee8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0B6N7tANPyVeBNmlSX19Ld2xDU1E'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "OYOUzkNGXl4t",
    "outputId": "e546e379-3039-4589-c06e-8e7535b4c8f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-17 18:01:22--  https://docs.google.com/uc?export=download&confirm=aW7L&id=0B6N7tANPyVeBNmlSX19Ld2xDU1E\n",
      "Resolving docs.google.com (docs.google.com)... 74.125.20.101, 74.125.20.102, 74.125.20.113, ...\n",
      "Connecting to docs.google.com (docs.google.com)|74.125.20.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0o-ac-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/q4r6poeq0hc5f39k16lstda30oco9atb/1574013600000/03129501499031348422/*/0B6N7tANPyVeBNmlSX19Ld2xDU1E?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2019-11-17 18:01:23--  https://doc-0o-ac-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/q4r6poeq0hc5f39k16lstda30oco9atb/1574013600000/03129501499031348422/*/0B6N7tANPyVeBNmlSX19Ld2xDU1E?e=download\n",
      "Resolving doc-0o-ac-docs.googleusercontent.com (doc-0o-ac-docs.googleusercontent.com)... 108.177.98.132, 2607:f8b0:400e:c06::84\n",
      "Connecting to doc-0o-ac-docs.googleusercontent.com (doc-0o-ac-docs.googleusercontent.com)|108.177.98.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/gzip]\n",
      "Saving to: ‘summary.tar.gz’\n",
      "\n",
      "summary.tar.gz          [               <=>  ] 277.39M  44.2MB/s    in 6.3s    \n",
      "\n",
      "2019-11-17 18:01:30 (44.2 MB/s) - ‘summary.tar.gz’ saved [290866023]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=0B6N7tANPyVeBNmlSX19Ld2xDU1E' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=0B6N7tANPyVeBNmlSX19Ld2xDU1E\" -O summary.tar.gz && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "LdA8FJ-YT3Jl",
    "outputId": "4d4eddd0-af2f-43c8-90ee-f6a80f31bd7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxrwxr-x srush/srush       0 2016-04-12 15:46 sumdata/\n",
      "drwxrwxr-x srush/srush       0 2016-04-12 15:27 sumdata/DUC2004/\n",
      "-rw-r--r-- srush/srush  103953 2016-04-12 15:27 sumdata/DUC2004/input.txt\n",
      "-rw-r--r-- srush/srush   36083 2016-04-12 15:27 sumdata/DUC2004/task1_ref3.txt\n",
      "-rw-r--r-- srush/srush   34426 2016-04-12 15:27 sumdata/DUC2004/task1_ref0.txt\n",
      "-rw-r--r-- srush/srush   35530 2016-04-12 15:27 sumdata/DUC2004/task1_ref1.txt\n",
      "-rw-r--r-- srush/srush   35967 2016-04-12 15:27 sumdata/DUC2004/task1_ref2.txt\n",
      "drwxrwxr-x srush/srush       0 2016-04-12 15:30 sumdata/Giga/\n",
      "-rw-rw-r-- srush/srush  335302 2016-04-12 15:25 sumdata/Giga/input.txt\n",
      "-rw-rw-r-- srush/srush  103766 2016-04-12 15:28 sumdata/Giga/task1_ref0.txt\n",
      "drwxrwxr-x srush/srush       0 2016-04-12 15:46 sumdata/DUC2003/\n",
      "-rw-r--r-- srush/srush  128364 2016-04-12 15:46 sumdata/DUC2003/input.txt\n",
      "-rw-r--r-- srush/srush   48910 2016-04-12 15:46 sumdata/DUC2003/task1_ref3.txt\n",
      "-rw-r--r-- srush/srush   43027 2016-04-12 15:46 sumdata/DUC2003/task1_ref0.txt\n",
      "-rw-r--r-- srush/srush   45454 2016-04-12 15:46 sumdata/DUC2003/task1_ref1.txt\n",
      "-rw-r--r-- srush/srush   45721 2016-04-12 15:46 sumdata/DUC2003/task1_ref2.txt\n",
      "drwxrwxr-x srush/srush       0 2016-04-12 15:28 sumdata/train/\n",
      "-rw-r--r-- srush/srush 64357057 2016-04-12 13:40 sumdata/train/train.title.txt.gz\n",
      "-rw-r--r-- srush/srush 34664441 2016-04-12 13:41 sumdata/train/valid.article.filter.txt\n",
      "-rw-r--r-- srush/srush 10046437 2016-04-12 13:41 sumdata/train/valid.title.filter.txt\n",
      "-rw-r--r-- srush/srush 211930097 2016-04-12 13:36 sumdata/train/train.article.txt.gz\n"
     ]
    }
   ],
   "source": [
    "!tar -tvf summary.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atSkLKu7YUbo"
   },
   "outputs": [],
   "source": [
    "!tar -xf summary.tar.gz sumdata/train/\n",
    "!gunzip sumdata/train/train.article.txt.gz\n",
    "!gunzip sumdata/train/train.title.txt.gz\n",
    "! mv sumdata/train/* Text-Summarizer-Pytorch/data/unfinished\n",
    "!rm -rf sumdata/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HPtC-lUgg60A",
    "outputId": "7ac4b8f5-26e4-43bf-d5dd-ce9b57ed80e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Text-Summarizer-Pytorch\n"
     ]
    }
   ],
   "source": [
    "%cd Text-Summarizer-Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "ULtgqMeehNiv",
    "outputId": "176bdd26-d7d2-453e-db60-c7bc691e14df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed shuffling train & valid text files\n",
      "3803957it [03:37, 17510.84it/s]\n",
      "189651it [00:03, 53459.22it/s]\n",
      "Completed creating bin file for train & valid\n",
      "Completed chunking main bin files into smaller ones\n"
     ]
    }
   ],
   "source": [
    "# Создание .bin файлов с данными для обучения модели\n",
    "!python2 make_data_files.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FK3MRVgZ2O5H"
   },
   "source": [
    "Примеры обучающих данных (заголовки и абстракты статей):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9VmxqtFmzKIO"
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "with open(\"data/unfinished/train.article.txt\") as f:\n",
    "    head = [next(f) for x in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "v0El6aWazoUA",
    "outputId": "8623aaf0-0142-4aef-d229-7a9c064eda13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at least two people were killed in a suspected bomb attack on a passenger bus in the strife-torn southern philippines on monday , the military said .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(head[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "vH46yo-8z1G4",
    "outputId": "0976fe03-c0c5-4207-b274-1788e747a997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at least two dead in southern philippines blast\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "with open(\"data/unfinished/train.title.txt\") as f:\n",
    "    head = [next(f) for x in range(N)]\n",
    "print(head[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "woOtaany2yVS"
   },
   "source": [
    "### Модель\n",
    "Идея состоит в обучении encoder-decoder архитектуры для генерации summary входного текста. \n",
    "\n",
    "В декодере дважды используется механизм attention:\n",
    "1. attention на состояния энкодера (intra-temporal attention) определяет вес слов входной последовательности для текущей позиции в выходной последовательности summary\n",
    "2. attention на предыдущие состояния декодера (intra-decoder attention) для того, чтобы не допускать повторения слов в выходе декодера.\n",
    "\n",
    "В процессе обучения модели используется teacher-forcing, чтобы учитывать ошибку на уровне каждого генерируемого слова (Negative Log Likelihood Loss), и reinforcement learning для оценки качества сгенерированного текста целиком в сравнении с target summary. \n",
    "\n",
    "Для reinforcement learning в качестве метрики используется ROUGE score. ROUGE считает совпадение н-грамм слов в таргете и сгенерированной последовательности (ROUGE-1 для униграмм, ROUGE-2 для биграмм слов, ...). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![summ_attentions](summ-attentions.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "hKDtgnWLmxlg",
    "outputId": "52e00fbc-8365-46a9-ee0c-8f5b35f5f94c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading https://files.pythonhosted.org/packages/63/ac/b93411318529980ab7f41e59ed64ec3ffed08ead32389e29eb78585dd55d/rouge-0.3.2-py3-none-any.whl\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-0.3.2\n"
     ]
    }
   ],
   "source": [
    "# rouge для подчета метрики Rouge\n",
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0c7KO5v3C730"
   },
   "source": [
    "Обучение модели (train.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XCMPOhhRCZ7Z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"    #Set cuda device\n",
    "\n",
    "import time\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from model import Model\n",
    "\n",
    "from data_util import config, data\n",
    "from data_util.batcher import Batcher\n",
    "from data_util.data import Vocab\n",
    "from train_util import *\n",
    "from torch.distributions import Categorical\n",
    "from rouge import Rouge\n",
    "from numpy import random\n",
    "import argparse\n",
    "\n",
    "random.seed(123)\n",
    "T.manual_seed(123)\n",
    "if T.cuda.is_available():\n",
    "    T.cuda.manual_seed_all(123)\n",
    "\n",
    "class Train(object):\n",
    "    def __init__(self, opt):\n",
    "        self.vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "        self.batcher = Batcher(config.train_data_path, self.vocab, mode='train',\n",
    "                               batch_size=config.batch_size, single_pass=False)\n",
    "        self.opt = opt\n",
    "        self.start_id = self.vocab.word2id(data.START_DECODING)\n",
    "        self.end_id = self.vocab.word2id(data.STOP_DECODING)\n",
    "        self.pad_id = self.vocab.word2id(data.PAD_TOKEN)\n",
    "        self.unk_id = self.vocab.word2id(data.UNKNOWN_TOKEN)\n",
    "        time.sleep(5)\n",
    "\n",
    "    def save_model(self, iter):\n",
    "        save_path = config.save_model_path + \"/%07d.tar\" % iter\n",
    "        T.save({\n",
    "            \"iter\": iter + 1,\n",
    "            \"model_dict\": self.model.state_dict(),\n",
    "            \"trainer_dict\": self.trainer.state_dict()\n",
    "        }, save_path)\n",
    "\n",
    "    def setup_train(self):\n",
    "        self.model = Model()\n",
    "        self.model = get_cuda(self.model)\n",
    "        self.trainer = T.optim.Adam(self.model.parameters(), lr=config.lr)\n",
    "        start_iter = 0\n",
    "        if self.opt.load_model is not None:\n",
    "            load_model_path = os.path.join(config.save_model_path, self.opt.load_model)\n",
    "            checkpoint = T.load(load_model_path)\n",
    "            start_iter = checkpoint[\"iter\"]\n",
    "            self.model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "            self.trainer.load_state_dict(checkpoint[\"trainer_dict\"])\n",
    "            print(\"Loaded model at \" + load_model_path)\n",
    "        if self.opt.new_lr is not None:\n",
    "            self.trainer = T.optim.Adam(self.model.parameters(), lr=self.opt.new_lr)\n",
    "        return start_iter\n",
    "\n",
    "    def train_batch_MLE(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, batch):\n",
    "        ''' Calculate Negative Log Likelihood Loss for the given batch. In order to reduce exposure bias,\n",
    "                pass the previous generated token as input with a probability of 0.25 instead of ground truth label\n",
    "        Args:\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param batch: batch object\n",
    "        '''\n",
    "        dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "        step_losses = []\n",
    "        s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))                             #Input to the decoder\n",
    "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "            use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "            x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
    "            x_t = self.model.embeds(x_t)\n",
    "            final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "            target = target_batch[:, t]\n",
    "            log_probs = T.log(final_dist + config.eps)\n",
    "            step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=self.pad_id)\n",
    "            step_losses.append(step_loss)\n",
    "            x_t = T.multinomial(final_dist, 1).squeeze()                                            #Sample words from final distribution which can be used as input in next time step\n",
    "            is_oov = (x_t >= config.vocab_size).long()                                              #Mask indicating whether sampled word is OOV\n",
    "            x_t = (1 - is_oov) * x_t.detach() + (is_oov) * self.unk_id                              #Replace OOVs with [UNK] token\n",
    "\n",
    "        losses = T.sum(T.stack(step_losses, 1), 1)                                                  #unnormalized losses for each example in the batch; (batch_size)\n",
    "        batch_avg_loss = losses / dec_lens                                                          #Normalized losses; (batch_size)\n",
    "        mle_loss = T.mean(batch_avg_loss)                                                           #Average batch loss\n",
    "        return mle_loss\n",
    "\n",
    "    def train_batch_RL(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, article_oovs, greedy):\n",
    "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
    "        Args\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param article_oovs: Batch containing list of OOVs in each example\n",
    "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
    "        Returns:\n",
    "        :decoded_strs: List of decoded sentences\n",
    "        :log_probs: Log probabilities of sampled words\n",
    "        '''\n",
    "        s_t = enc_hidden                                                                            #Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))                             #Input to the decoder\n",
    "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        inds = []                                                                                   #Stores sampled indices for each time step\n",
    "        decoder_padding_mask = []                                                                   #Stores padding masks of generated samples\n",
    "        log_probs = []                                                                              #Stores log probabilites of generated samples\n",
    "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(1))                                        #Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
    "\n",
    "        for t in range(config.max_dec_steps):\n",
    "            x_t = self.model.embeds(x_t)\n",
    "            probs, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "            if greedy is False:\n",
    "                multi_dist = Categorical(probs)\n",
    "                x_t = multi_dist.sample()                                                           #perform multinomial sampling\n",
    "                log_prob = multi_dist.log_prob(x_t)\n",
    "                log_probs.append(log_prob)\n",
    "            else:\n",
    "                _, x_t = T.max(probs, dim=1)                                                        #perform greedy sampling\n",
    "            x_t = x_t.detach()\n",
    "            inds.append(x_t)\n",
    "            mask_t = get_cuda(T.zeros(len(enc_out)))                                                #Padding mask of batch for current time step\n",
    "            mask_t[mask == 1] = 1                                                                   #If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
    "            mask[(mask == 1) + (x_t == self.end_id) == 2] = 0                                       #If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
    "            decoder_padding_mask.append(mask_t)\n",
    "            is_oov = (x_t>=config.vocab_size).long()                                                #Mask indicating whether sampled word is OOV\n",
    "            x_t = (1-is_oov)*x_t + (is_oov)*self.unk_id                                             #Replace OOVs with [UNK] token\n",
    "\n",
    "        inds = T.stack(inds, dim=1)\n",
    "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
    "        if greedy is False:                                                                         #If multinomial based sampling, compute log probabilites of sampled words\n",
    "            log_probs = T.stack(log_probs, dim=1)\n",
    "            log_probs = log_probs * decoder_padding_mask                                            #Not considering sampled words with padding mask = 0\n",
    "            lens = T.sum(decoder_padding_mask, dim=1)                                               #Length of sampled sentence\n",
    "            log_probs = T.sum(log_probs, dim=1) / lens  # (bs,)                                     #compute normalizied log probability of a sentence\n",
    "        decoded_strs = []\n",
    "        for i in range(len(enc_out)):\n",
    "            id_list = inds[i].cpu().numpy()\n",
    "            oovs = article_oovs[i]\n",
    "            S = data.outputids2words(id_list, self.vocab, oovs)                                     #Generate sentence corresponding to sampled words\n",
    "            try:\n",
    "                end_idx = S.index(data.STOP_DECODING)\n",
    "                S = S[:end_idx]\n",
    "            except ValueError:\n",
    "                S = S\n",
    "            if len(S) < 2:                                                                           #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "                S = [\"xxx\"]\n",
    "            S = \" \".join(S)\n",
    "            decoded_strs.append(S)\n",
    "\n",
    "        return decoded_strs, log_probs\n",
    "\n",
    "    def reward_function(self, decoded_sents, original_sents):\n",
    "        rouge = Rouge()\n",
    "        try:\n",
    "            scores = rouge.get_scores(decoded_sents, original_sents)\n",
    "        except Exception:\n",
    "            print(\"Rouge failed for multi sentence evaluation.. Finding exact pair\")\n",
    "            scores = []\n",
    "            for i in range(len(decoded_sents)):\n",
    "                try:\n",
    "                    score = rouge.get_scores(decoded_sents[i], original_sents[i])\n",
    "                except Exception:\n",
    "                    print(\"Error occured at:\")\n",
    "                    print(\"decoded_sents:\", decoded_sents[i])\n",
    "                    print(\"original_sents:\", original_sents[i])\n",
    "                    score = [{\"rouge-l\":{\"f\":0.0}}]\n",
    "                scores.append(score[0])\n",
    "        rouge_l_f1 = [score[\"rouge-l\"][\"f\"] for score in scores]\n",
    "        rouge_l_f1 = get_cuda(T.FloatTensor(rouge_l_f1))\n",
    "        return rouge_l_f1\n",
    "\n",
    "    # def write_to_file(self, decoded, max, original, sample_r, baseline_r, iter):\n",
    "    #     with open(\"temp.txt\", \"w\") as f:\n",
    "    #         f.write(\"iter:\"+str(iter)+\"\\n\")\n",
    "    #         for i in range(len(original)):\n",
    "    #             f.write(\"dec: \"+decoded[i]+\"\\n\")\n",
    "    #             f.write(\"max: \"+max[i]+\"\\n\")\n",
    "    #             f.write(\"org: \"+original[i]+\"\\n\")\n",
    "    #             f.write(\"Sample_R: %.4f, Baseline_R: %.4f\\n\\n\"%(sample_r[i].item(), baseline_r[i].item()))\n",
    "\n",
    "\n",
    "    def train_one_batch(self, batch, iter):\n",
    "        enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "\n",
    "        enc_batch = self.model.embeds(enc_batch)                                                    #Get embeddings for encoder input\n",
    "        enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "        # -------------------------------Summarization-----------------------\n",
    "        if self.opt.train_mle == \"yes\":                                                             #perform MLE training\n",
    "            mle_loss = self.train_batch_MLE(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch)\n",
    "        else:\n",
    "            mle_loss = get_cuda(T.FloatTensor([0]))\n",
    "        # --------------RL training-----------------------------------------------------\n",
    "        if self.opt.train_rl == \"yes\":                                                              #perform reinforcement learning training\n",
    "            # multinomial sampling\n",
    "            sample_sents, RL_log_probs = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch.art_oovs, greedy=False)\n",
    "            with T.autograd.no_grad():\n",
    "                # greedy sampling\n",
    "                greedy_sents, _ = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch.art_oovs, greedy=True)\n",
    "\n",
    "            sample_reward = self.reward_function(sample_sents, batch.original_abstracts)\n",
    "            baseline_reward = self.reward_function(greedy_sents, batch.original_abstracts)\n",
    "            # if iter%200 == 0:\n",
    "            #     self.write_to_file(sample_sents, greedy_sents, batch.original_abstracts, sample_reward, baseline_reward, iter)\n",
    "            rl_loss = -(sample_reward - baseline_reward) * RL_log_probs                             #Self-critic policy gradient training (eq 15 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "            rl_loss = T.mean(rl_loss)\n",
    "\n",
    "            batch_reward = T.mean(sample_reward).item()\n",
    "        else:\n",
    "            rl_loss = get_cuda(T.FloatTensor([0]))\n",
    "            batch_reward = 0\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "        self.trainer.zero_grad()\n",
    "        (self.opt.mle_weight * mle_loss + self.opt.rl_weight * rl_loss).backward()\n",
    "        self.trainer.step()\n",
    "\n",
    "        return mle_loss.item(), batch_reward\n",
    "\n",
    "    def trainIters(self):\n",
    "        iter = self.setup_train()\n",
    "        count = mle_total = r_total = 0\n",
    "        while iter <= config.max_iterations:\n",
    "            batch = self.batcher.next_batch()\n",
    "            try:\n",
    "                mle_loss, r = self.train_one_batch(batch, iter)\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"-------------------Keyboard Interrupt------------------\")\n",
    "                exit(0)\n",
    "\n",
    "            mle_total += mle_loss\n",
    "            r_total += r\n",
    "            count += 1\n",
    "            iter += 1\n",
    "\n",
    "            if iter % 1000 == 0:\n",
    "                mle_avg = mle_total / count\n",
    "                r_avg = r_total / count\n",
    "                print(\"iter:\", iter, \"mle_loss:\", \"%.3f\" % mle_avg, \"reward:\", \"%.4f\" % r_avg)\n",
    "                count = mle_total = r_total = 0\n",
    "\n",
    "            if iter % 5000 == 0:\n",
    "                self.save_model(iter)\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--train_mle', type=str, default=\"yes\")\n",
    "# parser.add_argument('--train_rl', type=str, default=\"no\")\n",
    "# parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "# parser.add_argument('--load_model', type=str, default=None)\n",
    "# parser.add_argument('--new_lr', type=float, default=None)\n",
    "# opt = parser.parse_args()\n",
    "# opt.rl_weight = 1 - opt.mle_weight\n",
    "# print(\"Training mle: %s, Training rl: %s, mle weight: %.2f, rl weight: %.2f\"%(opt.train_mle, opt.train_rl, opt.mle_weight, opt.rl_weight))\n",
    "# print(\"intra_encoder:\", config.intra_encoder, \"intra_decoder:\", config.intra_decoder)\n",
    "\n",
    "# train_processor = Train(opt)\n",
    "# train_processor.trainIters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D4SsF_7BDQwF"
   },
   "source": [
    "Сначала encoder-decoder модель бучается без reinforcement learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "1kUSZGFojE9K",
    "outputId": "783c693d-6d44-4eaf-a99c-54066f97d128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mle: yes, Training rl: no, mle weight: 1.00, rl weight: 0.00\n",
      "intra_encoder: True intra_decoder: True\n",
      "WARNING:tensorflow:From /content/Text-Summarizer-Pytorch/data_util/batcher.py:247: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "iter: 1000 mle_loss: 4.683 reward: 0.0000\n",
      "-------------------Keyboard Interrupt------------------\n",
      "Training mle: yes, Training rl: no, mle weight: 1.00, rl weight: 0.00\n",
      "intra_encoder: True intra_decoder: True\n",
      "WARNING:tensorflow:From /content/Text-Summarizer-Pytorch/data_util/batcher.py:247: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "iter: 1000 mle_loss: 4.683 reward: 0.0000\n",
      "-------------------Keyboard Interrupt------------------\n"
     ]
    }
   ],
   "source": [
    "!python train.py --train_mle=yes --train_rl=no --mle_weight=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r049heCmGAIT"
   },
   "source": [
    "Выбирается лучшая модель (из обученных с разным числом итераций) по значению ROUGE на валидационной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TStUf1SLmh4l"
   },
   "outputs": [],
   "source": [
    "!python eval.py --task=validate --start_from=0005000.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTq3MdGtGE01"
   },
   "source": [
    "Лучшая модель дообучается с использованием RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gu-XLAm8Fe63"
   },
   "outputs": [],
   "source": [
    "# MLE + RL training\n",
    "!python train.py --train_mle=yes --train_rl=yes --mle_weight=0.25 --load_model=0100000.tar --new_lr=0.0001 \n",
    "\n",
    "# RL training\n",
    "!python train.py --train_mle=no --train_rl=yes --mle_weight=0.0 --load_model=0100000.tar --new_lr=0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vB-YSeGhGnrK"
   },
   "source": [
    "Модель, обученная только на RL, достигает более высоких показателей ROUGE, но генерирует менее хорошие тексты с точки зрения связности и естественности, поэтому авторы статьи рекомендуют комбинированную стратегию обучения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVIZJv5uH7i_"
   },
   "source": [
    "* Результаты, приведенные авторами репозитория:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nO0bhu9wHgGn"
   },
   "source": [
    "Rouge scores obtained by using best MLE trained model on test set:\n",
    "\n",
    "{\n",
    "'rouge-1': {'f': 0.4412018559893622, 'p': 0.4814799494024485, 'r': 0.4232331027817015}, \n",
    "\n",
    "'rouge-2': {'f': 0.23238981595683728, 'p': 0.2531296070596062, 'r': 0.22407861554997008},\n",
    "\n",
    "'rouge-l': {'f': 0.40477682528278364, 'p': 0.4584684491434479, 'r': 0.40351107200202596}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PeUQzzvNHzPc"
   },
   "source": [
    "Rouge scores obtained by using best MLE + RL trained model on test set:\n",
    "\n",
    "{\n",
    "'rouge-1': {'f': 0.4499047033247696, 'p': 0.4853756369556345, 'r': 0.43544461386607497},\n",
    "\n",
    "'rouge-2': {'f': 0.24037014314625643, 'p': 0.25903387205387235, 'r': 0.23362662645146298},\n",
    "\n",
    "'rouge-l': {'f': 0.41320241732946406, 'p': 0.4616655167980162, 'r': 0.4144419466382236}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiFAGr6aIbIN"
   },
   "source": [
    "* Примеры (article - исходный текст, ref - target summary, dec - сгенерированный моделью текст):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KsCPWCkTIa8M"
   },
   "source": [
    "article: russia 's lower house of parliament was scheduled friday to debate an appeal to the prime minister that challenged the right of u.s.-funded radio liberty to operate in russia following its introduction of broadcasts targeting chechnya .\n",
    "\n",
    "ref: russia 's lower house of parliament mulls challenge to radio liberty\n",
    "\n",
    "dec: russian parliament to debate on banning radio liberty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JaW__-pIJBYT"
   },
   "source": [
    "article: continued dialogue with the democratic people 's republic of korea is important although australia 's plan to open its embassy in pyongyang has been shelved because of the crisis over the dprk 's nuclear weapons program , australian foreign minister alexander downer said on friday .\n",
    "\n",
    "ref: dialogue with dprk important says australian foreign minister\n",
    "\n",
    "dec: australian fm says dialogue with dprk important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PkMDr8zAJW3o"
   },
   "source": [
    "article: water levels in the zambezi river are rising due to heavy rains in its catchment area , prompting zimbabwe 's civil protection unit -lrb- cpu -rrb- to issue a flood alert for people living in the zambezi valley , the herald reported on friday .\n",
    "\n",
    "ref: floods loom in zambezi valley\n",
    "\n",
    "dec: water levels rising in zambezi river"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i2KBeWCXJvT0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4W38g90JvAS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7VcvVX9YJon0"
   },
   "source": [
    "## BERT Extractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCOikZ1yJyDo"
   },
   "source": [
    "### Источник:\n",
    "https://deeplearninganalytics.org/text-summarization/\n",
    "\n",
    "https://github.com/nlpyang/BertSum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aPWErqGiSpGZ"
   },
   "source": [
    "Идея: использовать BERT эмбеддинги предложений исходного текста в задаче бинарной классификации для отбора самых значимых предложений, которые войдут в summary.\n",
    "\n",
    "Для получения эмбеддингов нескольких предложений текста перед каждым предложением текста вставляется свой символ начала предложения **[CLS]**, после каждого предложения - символ **[SEP]**. В качестве эмебддингов сегмента предложения (которые используются для того, чтобы различать первое и второе предложения в парах предложений при обучении  BERT) для последовательности предложений чередуются единичные и нулевые вектора.\n",
    "\n",
    "_[sent1, sent2, sent3, sent4, sent5] -> [EA, EB, EA, EB, EA]._\n",
    "\n",
    "Вектора символов [CLS] на последнем слое BERT используются в качестве векторов предложений текста. Вектора предложений подаются на вход классификатору (в статье 3 варианта классификации: \n",
    "1. linear layer + sigmoid\n",
    "2. Transformer + sigmoid\n",
    "3. LSTM + sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bertsum](bertsum.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "qgVjUukfgYAr",
    "outputId": "ba29d844-7301-4689-9ab4-0d2260df3a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/60/f685fb2cfb3088736bafbc9bdbb455327bdc8906b606da9c9a81bae1c81e/torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (676.9MB)\n",
      "\u001b[K     |████████████████████████████████| 676.9MB 25kB/s \n",
      "\u001b[?25hCollecting numpy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/ab/43e678759326f728de861edbef34b8e2ad1b1490505f20e0d1f0716c3bf4/numpy-1.17.4-cp36-cp36m-manylinux1_x86_64.whl (20.0MB)\n",
      "\u001b[K     |████████████████████████████████| 20.0MB 441kB/s \n",
      "\u001b[31mERROR: torchvision 0.4.2+cu100 has requirement torch==1.3.1, but you'll have torch 1.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, torch\n",
      "  Found existing installation: numpy 1.17.4\n",
      "    Uninstalling numpy-1.17.4:\n",
      "      Successfully uninstalled numpy-1.17.4\n",
      "  Found existing installation: torch 1.3.1+cu100\n",
      "    Uninstalling torch-1.3.1+cu100:\n",
      "      Successfully uninstalled torch-1.3.1+cu100\n",
      "Successfully installed numpy-1.17.4 torch-1.1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy",
         "torch"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --force-reinstall torch==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "I3rNua2xJsxN",
    "outputId": "1a53ebc5-cf2d-4525-c096-934d166affbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-pretrained-bert\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 9.5MB/s \n",
      "\u001b[?25hCollecting regex\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 59.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.10.14)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.1.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.14 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.13.14)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->pytorch-pretrained-bert) (2.6.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->pytorch-pretrained-bert) (0.15.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.14->boto3->pytorch-pretrained-bert) (1.12.0)\n",
      "Installing collected packages: regex, pytorch-pretrained-bert\n",
      "Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "px7VPlV8YOxq",
    "outputId": "0cf160ab-a326-4ca0-e93c-d090b0c0fb85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n",
      "\r",
      "\u001b[K     |█▊                              | 10kB 28.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 20kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 30kB 9.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 40kB 6.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 51kB 7.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 61kB 8.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 71kB 9.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 81kB 11.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 92kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 102kB 9.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 112kB 9.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 122kB 9.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 133kB 9.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 143kB 9.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 153kB 9.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 163kB 9.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 174kB 9.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 184kB 9.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 194kB 9.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (41.4.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "qfCY-rAGYy5E",
    "outputId": "adfea496-0655-44d2-ce17-d082ecc90fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyrouge\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/85/e522dd6b36880ca19dcf7f262b22365748f56edc6f455e7b6a37d0382c32/pyrouge-0.1.3.tar.gz (60kB)\n",
      "\r",
      "\u001b[K     |█████▍                          | 10kB 27.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 20kB 6.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 30kB 8.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 40kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 51kB 6.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 61kB 4.6MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyrouge\n",
      "  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyrouge: filename=pyrouge-0.1.3-cp36-none-any.whl size=191613 sha256=bbe3322481dda78ee16471b8e91242f5147764f9e90ca74ec635a4e2339a3eda\n",
      "  Stored in directory: /root/.cache/pip/wheels/75/d3/0c/e5b04e15b6b87c42e980de3931d2686e14d36e045058983599\n",
      "Successfully built pyrouge\n",
      "Installing collected packages: pyrouge\n",
      "Successfully installed pyrouge-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pyrouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iX0FyN5nY9le",
    "outputId": "2bb3b687-f112-4433-d008-8a874905a1c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (0.70.9)\n",
      "Requirement already satisfied: dill>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from multiprocess) (0.3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install multiprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D5M17ucVaPkf"
   },
   "source": [
    "### Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpH5yBI4ckF_"
   },
   "source": [
    "Датасет CNN and Daily Mail \n",
    "\n",
    "Загрузим предобработанные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "77qiS30hafOT",
    "outputId": "4cecc8df-451a-4419-c70a-a46742b6779f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "9uXL_7cVkV-f",
    "outputId": "04a70e3f-6261-46a4-ef63-32a21e23cdf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-18 08:12:22--  http://docs.google.com/uc?export=download&confirm=CTBV&id=1WE9ZHAW64zKfU41KmUXsLda7zDfT9EqN\n",
      "Resolving docs.google.com (docs.google.com)... 74.125.143.100, 74.125.143.113, 74.125.143.138, ...\n",
      "Connecting to docs.google.com (docs.google.com)|74.125.143.100|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://docs.google.com/uc?export=download&confirm=CTBV&id=1WE9ZHAW64zKfU41KmUXsLda7zDfT9EqN [following]\n",
      "--2019-11-18 08:12:22--  https://docs.google.com/uc?export=download&confirm=CTBV&id=1WE9ZHAW64zKfU41KmUXsLda7zDfT9EqN\n",
      "Connecting to docs.google.com (docs.google.com)|74.125.143.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0g-6k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lq5e38ndfkdtlb1v61iiki230onqvskv/1574064000000/04043709603932948010/*/1WE9ZHAW64zKfU41KmUXsLda7zDfT9EqN?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2019-11-18 08:12:22--  https://doc-0g-6k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lq5e38ndfkdtlb1v61iiki230onqvskv/1574064000000/04043709603932948010/*/1WE9ZHAW64zKfU41KmUXsLda7zDfT9EqN?e=download\n",
      "Resolving doc-0g-6k-docs.googleusercontent.com (doc-0g-6k-docs.googleusercontent.com)... 108.177.127.132, 2a00:1450:4013:c07::84\n",
      "Connecting to doc-0g-6k-docs.googleusercontent.com (doc-0g-6k-docs.googleusercontent.com)|108.177.127.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘bertsum_data.zip’\n",
      "\n",
      "bertsum_data.zip        [  <=>               ] 829.12M  60.4MB/s    in 16s     \n",
      "\n",
      "2019-11-18 08:12:39 (52.9 MB/s) - ‘bertsum_data.zip’ saved [869392410]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget --no-check-certificate --load-cookies /tmp/cookies.txt \"http://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'http://docs.google.com/uc?export=download&id=1WE9ZHAW64zKfU41KmUXsLda7zDfT9EqN' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1WE9ZHAW64zKfU41KmUXsLda7zDfT9EqN\" -O bertsum_data.zip && rm -rf /tmp/cookies.txt\n",
    "!wget --no-check-certificate --load-cookies /tmp/cookies.txt \"http://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'http://docs.google.com/uc?export=download&id=1x0d61LP9UAN389YN00z0Pv-7jQgirVg6' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1x0d61LP9UAN389YN00z0Pv-7jQgirVg6\" -O bertsum_data.zip && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "lK31E4MAdCEH",
    "outputId": "e5038d96-ebbf-46ee-a7b0-c28c509a3fc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'BertSum'...\n",
      "remote: Enumerating objects: 301, done.\u001b[K\n",
      "remote: Total 301 (delta 0), reused 0 (delta 0), pack-reused 301\n",
      "Receiving objects: 100% (301/301), 15.03 MiB | 13.32 MiB/s, done.\n",
      "Resolving deltas: 100% (174/174), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nlpyang/BertSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0ERlE4EYdN9K",
    "outputId": "4a6597a6-6ea8-4fdd-84a8-d911635e6657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/BertSum\n"
     ]
    }
   ],
   "source": [
    "%cd BertSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Yo0WKWbWiPJ5",
    "outputId": "d9b98c78-2c2b-42ea-e350-d781f1576ee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../bertsum_data.zip\n",
      "  inflating: ./bert_data/cnndm.test.0.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.test.1.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.test.2.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.test.3.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.test.4.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.test.5.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.0.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.100.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.101.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.102.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.103.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.104.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.105.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.106.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.107.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.108.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.109.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.10.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.110.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.111.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.112.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.113.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.114.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.115.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.116.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.117.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.118.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.119.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.11.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.120.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.121.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.122.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.123.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.124.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.125.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.126.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.127.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.128.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.129.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.12.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.130.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.131.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.132.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.133.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.134.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.135.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.136.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.137.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.138.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.139.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.13.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.140.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.141.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.142.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.143.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.14.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.15.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.16.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.17.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.18.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.19.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.1.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.20.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.21.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.22.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.23.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.24.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.25.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.26.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.27.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.28.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.29.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.2.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.30.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.31.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.32.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.33.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.34.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.35.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.36.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.37.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.38.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.39.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.3.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.40.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.41.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.42.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.43.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.44.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.45.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.46.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.47.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.48.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.49.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.4.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.50.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.51.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.52.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.53.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.54.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.55.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.56.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.57.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.58.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.59.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.5.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.60.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.61.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.62.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.63.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.64.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.65.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.66.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.67.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.68.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.69.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.6.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.70.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.71.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.72.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.73.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.74.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.75.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.76.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.77.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.78.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.79.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.7.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.80.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.81.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.82.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.83.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.84.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.85.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.86.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.87.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.88.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.89.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.8.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.90.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.91.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.92.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.93.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.94.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.95.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.96.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.97.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.98.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.99.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.train.9.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.valid.0.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.valid.1.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.valid.2.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.valid.3.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.valid.4.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.valid.5.bert.pt  \n",
      "  inflating: ./bert_data/cnndm.valid.6.bert.pt  \n"
     ]
    }
   ],
   "source": [
    "!unzip ../bertsum_data.zip -d ./bert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w8rHG49yedkF",
    "outputId": "c8cadffa-9201-4d6f-dc13-8583826bac44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/BertSum/src\n"
     ]
    }
   ],
   "source": [
    "%cd src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHqhIu_LsNeG"
   },
   "source": [
    "Пример входных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLDitZ3mnv_J"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "cnn_test_samp = torch.load(\"/content/BertSum/bert_data/cnndm.test.0.bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jR9kyPftqoWv"
   },
   "outputs": [],
   "source": [
    "cnn_test_samp0 = cnn_test_samp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Lx4X9QDEq_uX",
    "outputId": "79f3f6c8-dd45-4ccc-82fa-fb992aa96022"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['src', 'labels', 'segs', 'clss', 'src_txt', 'tgt_txt'])"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_test_samp0.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "RydoDsS6oQ6g",
    "outputId": "8660f619-d9f7-4325-9bd5-f429d354da54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 25, 57, 78, 112, 136, 174, 197, 223, 245, 285, 301, 337, 358, 382, 416, 452]\n",
      "[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[101, 1037, 2118, 1997, 5947, 3076, 2038, 2351, 3053, 2093, 2706, 2044, 1037, 2991, 1999, 4199, 1999, 1037, 6878, 13742, 2886, 1999, 4199, 1012, 102, 101, 4080, 9587, 29076, 1010, 2322, 1010, 2013, 8904, 3449, 9644, 1010, 4307, 1010, 2018, 2069, 2074, 3369, 2005, 1037, 13609, 2565, 1999, 3304, 2043, 1996, 5043, 3047, 1999, 2254, 1012, 102, 101, 2002, 2001, 10583, 2067, 2000, 3190, 3081, 2250, 10771, 2006, 2233, 2322, 1010, 2021, 2002, 2351, 2006, 4465, 1012, 102, 101, 4080, 9587, 29076, 1010, 2322, 1010, 2013, 8904, 3449, 9644, 1010, 4307, 1010, 1037, 2118, 1997, 5947, 3076, 2038, 2351, 3053, 2093, 2706, 2044, 1037, 2991, 1999, 4199, 1999, 1037, 6878, 13742, 102, 101, 2002, 2001, 2579, 2000, 1037, 2966, 4322, 1999, 1996, 3190, 2181, 1010, 2485, 2000, 2010, 2155, 2188, 1999, 8904, 3449, 9644, 1012, 102, 101, 2002, 2351, 2006, 4465, 2012, 7855, 3986, 2902, 1011, 2966, 19684, 1005, 1055, 2436, 14056, 3581, 18454, 6199, 2319, 2758, 1037, 3426, 1997, 2331, 24185, 1050, 1005, 1056, 2022, 2207, 2127, 6928, 2012, 1996, 5700, 1012, 102, 101, 3988, 2610, 4311, 5393, 1996, 2991, 2001, 2019, 4926, 2021, 4614, 2024, 11538, 1996, 6061, 2008, 9587, 29076, 2001, 20114, 1012, 102, 101, 2006, 4465, 1010, 2010, 5542, 9460, 2626, 3784, 1024, 1036, 2023, 2851, 2026, 5542, 4080, 1005, 1055, 3969, 2001, 4196, 2039, 2000, 6014, 1012, 102, 101, 3988, 2610, 4311, 5393, 1996, 2991, 2001, 2019, 4926, 2021, 4614, 2024, 11538, 1996, 6061, 2008, 9587, 29076, 2001, 20114, 102, 101, 1036, 2012, 1996, 2927, 1997, 2254, 2002, 2253, 2000, 4199, 2000, 2817, 7548, 1998, 2006, 1996, 2126, 2188, 2013, 1037, 2283, 2002, 2001, 23197, 4457, 1998, 6908, 2125, 1037, 2871, 6199, 2958, 1998, 2718, 1996, 5509, 2917, 1012, 102, 101, 1036, 2002, 2001, 1999, 1037, 16571, 1998, 1999, 4187, 4650, 2005, 2706, 1012, 1005, 102, 101, 13723, 20073, 1010, 2040, 2056, 2016, 2003, 1037, 2485, 2155, 2767, 1010, 2409, 2026, 9282, 2166, 1010, 2008, 9587, 29076, 2018, 2069, 2042, 1999, 1996, 2406, 2005, 2416, 2847, 2043, 1996, 5043, 3047, 1012, 102, 101, 2016, 2056, 2002, 2001, 2001, 2894, 2012, 1996, 2051, 1997, 1996, 6884, 6101, 1998, 3167, 5167, 2020, 7376, 1012, 102, 101, 2016, 2794, 2008, 2002, 2001, 1999, 1037, 2512, 1011, 2966, 2135, 10572, 16571, 1010, 2383, 4265, 3809, 8985, 1998, 4722, 9524, 1012, 102, 101, 9587, 29076, 2001, 1037, 2353, 1011, 2095, 5446, 2350, 2013, 8904, 3449, 9644, 1010, 5665, 1012, 1010, 2040, 2001, 8019, 1999, 1037, 13609, 1011, 2146, 2565, 2012, 2198, 9298, 4140, 2118, 1012, 102, 101, 9587, 29076, 6272, 2000, 1996, 2082, 1005, 1055, 3127, 1997, 1996, 13201, 16371, 13577, 1010, 4311, 1996, 3190, 10969, 2040, 6866, 1037, 3696, 2648, 1037, 2311, 3752, 1036, 11839, 2005, 9587, 29076, 1012, 1005, 102, 101, 1996, 13577, 1005, 1055, 5947, 3127, 2623, 4465, 5027, 3081, 10474, 2008, 1037, 3986, 2326, 2097, 2022, 2218, 2006, 3721, 2000, 3342, 9587, 29076, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "print(cnn_test_samp0['clss']) # индексы CLS токенов для предложений входного текста \n",
    "print(cnn_test_samp0['labels']) # таргет метки для предложений (1 - входит в summary, 0 - не входит)\n",
    "print(cnn_test_samp0['segs']) # id сегментов предложений \n",
    "print(cnn_test_samp0['src']) # id слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "Xs7zKGfYrM0i",
    "outputId": "d7cde693-ce33-4342-ba57-daa641bfea52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .',\n",
       " 'he was flown back to chicago via air ambulance on march 20 , but he died on sunday .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , a university of iowa student has died nearly three months after a fall in rome in a suspected robbery',\n",
       " 'he was taken to a medical facility in the chicago area , close to his family home in glen ellyn .',\n",
       " \"he died on sunday at northwestern memorial hospital - medical examiner 's office spokesman frank shuftan says a cause of death wo n't be released until monday at the earliest .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed .',\n",
       " \"on sunday , his cousin abby wrote online : ` this morning my cousin andrew 's soul was lifted up to heaven .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed',\n",
       " '` at the beginning of january he went to rome to study aboard and on the way home from a party he was brutally attacked and thrown off a 40ft bridge and hit the concrete below .',\n",
       " \"` he was in a coma and in critical condition for months . '\",\n",
       " 'paula barnett , who said she is a close family friend , told my suburban life , that mogni had only been in the country for six hours when the incident happened .',\n",
       " 'she said he was was alone at the time of the alleged assault and personal items were stolen .',\n",
       " 'she added that he was in a non-medically induced coma , having suffered serious infection and internal bleeding .',\n",
       " 'mogni was a third-year finance major from glen ellyn , ill. , who was participating in a semester-long program at john cabot university .',\n",
       " \"mogni belonged to the school 's chapter of the sigma nu fraternity , reports the chicago tribune who posted a sign outside a building reading ` pray for mogni . '\",\n",
       " \"the fraternity 's iowa chapter announced sunday afternoon via twitter that a memorial service will be held on campus to remember mogni .\"]"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_test_samp0['src_txt'] # входной текст\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "-DSbhBlXrRWH",
    "outputId": "3f526a7f-1701-4b5f-86cb-9249412893c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_test_samp0['tgt_txt'] # target summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PW5EKEL8d9GG",
    "outputId": "3bc406f8-96ce-44de-d945-23554a9d721a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-11-18 09:02:49,440 INFO] Device ID 0\n",
      "[2019-11-18 09:02:49,440 INFO] Device cuda\n",
      "[2019-11-18 09:02:49,880 INFO] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "[2019-11-18 09:02:49,880 INFO] extracting archive file ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpx3w7yrtg\n",
      "[2019-11-18 09:02:53,727 INFO] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2019-11-18 09:02:59,980 INFO] Summarizer(\n",
      "  (bert): Bert(\n",
      "    (model): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): BertLayerNorm()\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Classifier(\n",
      "    (linear1): Linear(in_features=768, out_features=1, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "gpu_rank 0\n",
      "[2019-11-18 09:03:00,052 INFO] * number of parameters: 109483009\n",
      "[2019-11-18 09:03:00,052 INFO] Start training...\n",
      "[2019-11-18 09:03:00,150 INFO] Loading train dataset from ../bert_data/cnndm.train.123.bert.pt, number of examples: 2001\n",
      "[2019-11-18 09:03:33,899 INFO] Step 50/ 5000; xent: 7.34; lr: 0.0000001;  30 docs/s;     34 sec\n",
      "[2019-11-18 09:04:07,662 INFO] Step 100/ 5000; xent: 6.33; lr: 0.0000002;  30 docs/s;     68 sec\n",
      "[2019-11-18 09:04:41,554 INFO] Step 150/ 5000; xent: 4.87; lr: 0.0000003;  30 docs/s;    101 sec\n",
      "[2019-11-18 09:05:14,840 INFO] Loading train dataset from ../bert_data/cnndm.train.91.bert.pt, number of examples: 1998\n",
      "[2019-11-18 09:05:15,555 INFO] Step 200/ 5000; xent: 3.80; lr: 0.0000004;  30 docs/s;    135 sec\n",
      "[2019-11-18 09:05:49,044 INFO] Step 250/ 5000; xent: 3.42; lr: 0.0000005;  30 docs/s;    169 sec\n",
      "[2019-11-18 09:06:22,954 INFO] Step 300/ 5000; xent: 3.31; lr: 0.0000006;  30 docs/s;    203 sec\n",
      "[2019-11-18 09:06:56,150 INFO] Step 350/ 5000; xent: 3.31; lr: 0.0000007;  30 docs/s;    236 sec\n",
      "[2019-11-18 09:07:28,609 INFO] Loading train dataset from ../bert_data/cnndm.train.39.bert.pt, number of examples: 2000\n",
      "[2019-11-18 09:07:29,931 INFO] Step 400/ 5000; xent: 3.30; lr: 0.0000008;  30 docs/s;    270 sec\n",
      "[2019-11-18 09:08:03,689 INFO] Step 450/ 5000; xent: 3.32; lr: 0.0000009;  30 docs/s;    304 sec\n",
      "[2019-11-18 09:08:37,412 INFO] Step 500/ 5000; xent: 3.24; lr: 0.0000010;  30 docs/s;    337 sec\n",
      "[2019-11-18 09:09:11,012 INFO] Step 550/ 5000; xent: 3.30; lr: 0.0000011;  30 docs/s;    371 sec\n",
      "[2019-11-18 09:09:43,066 INFO] Loading train dataset from ../bert_data/cnndm.train.6.bert.pt, number of examples: 2001\n",
      "[2019-11-18 09:09:45,135 INFO] Step 600/ 5000; xent: 3.31; lr: 0.0000012;  30 docs/s;    405 sec\n",
      "[2019-11-18 09:10:18,849 INFO] Step 650/ 5000; xent: 3.32; lr: 0.0000013;  30 docs/s;    439 sec\n",
      "[2019-11-18 09:10:52,626 INFO] Step 700/ 5000; xent: 3.29; lr: 0.0000014;  30 docs/s;    472 sec\n",
      "[2019-11-18 09:11:26,295 INFO] Step 750/ 5000; xent: 3.33; lr: 0.0000015;  30 docs/s;    506 sec\n",
      "[2019-11-18 09:11:56,875 INFO] Loading train dataset from ../bert_data/cnndm.train.81.bert.pt, number of examples: 2000\n",
      "[2019-11-18 09:12:00,305 INFO] Step 800/ 5000; xent: 3.30; lr: 0.0000016;  30 docs/s;    540 sec\n",
      "[2019-11-18 09:12:34,050 INFO] Step 850/ 5000; xent: 3.27; lr: 0.0000017;  30 docs/s;    574 sec\n",
      "[2019-11-18 09:13:07,505 INFO] Step 900/ 5000; xent: 3.30; lr: 0.0000018;  30 docs/s;    607 sec\n",
      "[2019-11-18 09:13:41,193 INFO] Step 950/ 5000; xent: 3.22; lr: 0.0000019;  30 docs/s;    641 sec\n",
      "[2019-11-18 09:14:11,123 INFO] Loading train dataset from ../bert_data/cnndm.train.98.bert.pt, number of examples: 2000\n",
      "[2019-11-18 09:14:15,255 INFO] Step 1000/ 5000; xent: 3.21; lr: 0.0000020;  30 docs/s;    675 sec\n",
      "[2019-11-18 09:14:15,257 INFO] Saving checkpoint ../models/bert_classifier/model_step_1000.pt\n",
      "[2019-11-18 09:14:53,559 INFO] Step 1050/ 5000; xent: 3.38; lr: 0.0000021;  26 docs/s;    713 sec\n",
      "[2019-11-18 09:15:27,343 INFO] Step 1100/ 5000; xent: 3.24; lr: 0.0000022;  30 docs/s;    747 sec\n",
      "[2019-11-18 09:16:01,054 INFO] Step 1150/ 5000; xent: 3.29; lr: 0.0000023;  30 docs/s;    781 sec\n",
      "[2019-11-18 09:16:29,537 INFO] Loading train dataset from ../bert_data/cnndm.train.93.bert.pt, number of examples: 1997\n",
      "[2019-11-18 09:16:34,997 INFO] Step 1200/ 5000; xent: 3.24; lr: 0.0000024;  30 docs/s;    815 sec\n",
      "[2019-11-18 09:17:08,685 INFO] Step 1250/ 5000; xent: 3.21; lr: 0.0000025;  30 docs/s;    849 sec\n",
      "[2019-11-18 09:17:42,317 INFO] Step 1300/ 5000; xent: 3.19; lr: 0.0000026;  30 docs/s;    882 sec\n",
      "[2019-11-18 09:18:15,783 INFO] Step 1350/ 5000; xent: 3.13; lr: 0.0000027;  30 docs/s;    916 sec\n",
      "[2019-11-18 09:18:43,166 INFO] Loading train dataset from ../bert_data/cnndm.train.63.bert.pt, number of examples: 2001\n",
      "[2019-11-18 09:18:49,979 INFO] Step 1400/ 5000; xent: 3.14; lr: 0.0000028;  30 docs/s;    950 sec\n",
      "[2019-11-18 09:19:23,571 INFO] Step 1450/ 5000; xent: 3.10; lr: 0.0000029;  30 docs/s;    983 sec\n",
      "[2019-11-18 09:19:57,158 INFO] Step 1500/ 5000; xent: 3.10; lr: 0.0000030;  30 docs/s;   1017 sec\n",
      "[2019-11-18 09:20:30,886 INFO] Step 1550/ 5000; xent: 3.11; lr: 0.0000031;  30 docs/s;   1051 sec\n",
      "[2019-11-18 09:20:57,157 INFO] Loading train dataset from ../bert_data/cnndm.train.15.bert.pt, number of examples: 1999\n",
      "[2019-11-18 09:21:04,646 INFO] Step 1600/ 5000; xent: 3.17; lr: 0.0000032;  30 docs/s;   1084 sec\n",
      "[2019-11-18 09:21:38,023 INFO] Step 1650/ 5000; xent: 3.09; lr: 0.0000033;  30 docs/s;   1118 sec\n",
      "[2019-11-18 09:22:11,859 INFO] Step 1700/ 5000; xent: 3.09; lr: 0.0000034;  30 docs/s;   1152 sec\n",
      "[2019-11-18 09:22:45,337 INFO] Step 1750/ 5000; xent: 3.05; lr: 0.0000035;  30 docs/s;   1185 sec\n",
      "[2019-11-18 09:23:10,906 INFO] Loading train dataset from ../bert_data/cnndm.train.64.bert.pt, number of examples: 2001\n",
      "[2019-11-18 09:23:19,046 INFO] Step 1800/ 5000; xent: 3.02; lr: 0.0000036;  30 docs/s;   1219 sec\n",
      "[2019-11-18 09:23:52,705 INFO] Step 1850/ 5000; xent: 3.16; lr: 0.0000037;  30 docs/s;   1253 sec\n",
      "[2019-11-18 09:24:26,626 INFO] Step 1900/ 5000; xent: 3.00; lr: 0.0000038;  30 docs/s;   1286 sec\n",
      "[2019-11-18 09:25:00,188 INFO] Step 1950/ 5000; xent: 3.02; lr: 0.0000039;  30 docs/s;   1320 sec\n",
      "[2019-11-18 09:25:24,635 INFO] Loading train dataset from ../bert_data/cnndm.train.28.bert.pt, number of examples: 2000\n",
      "[2019-11-18 09:25:34,157 INFO] Step 2000/ 5000; xent: 3.03; lr: 0.0000040;  30 docs/s;   1354 sec\n",
      "[2019-11-18 09:25:34,158 INFO] Saving checkpoint ../models/bert_classifier/model_step_2000.pt\n",
      "[2019-11-18 09:26:12,453 INFO] Step 2050/ 5000; xent: 3.00; lr: 0.0000041;  27 docs/s;   1392 sec\n",
      "[2019-11-18 09:26:46,106 INFO] Step 2100/ 5000; xent: 3.10; lr: 0.0000042;  30 docs/s;   1426 sec\n",
      "[2019-11-18 09:27:19,176 INFO] Step 2150/ 5000; xent: 3.05; lr: 0.0000043;  30 docs/s;   1459 sec\n",
      "[2019-11-18 09:27:43,762 INFO] Loading train dataset from ../bert_data/cnndm.train.32.bert.pt, number of examples: 1998\n",
      "[2019-11-18 09:27:53,268 INFO] Step 2200/ 5000; xent: 3.06; lr: 0.0000044;  30 docs/s;   1493 sec\n",
      "[2019-11-18 09:28:26,835 INFO] Step 2250/ 5000; xent: 3.01; lr: 0.0000045;  30 docs/s;   1527 sec\n",
      "[2019-11-18 09:29:00,572 INFO] Step 2300/ 5000; xent: 3.08; lr: 0.0000046;  30 docs/s;   1560 sec\n",
      "[2019-11-18 09:29:33,995 INFO] Step 2350/ 5000; xent: 3.15; lr: 0.0000047;  30 docs/s;   1594 sec\n",
      "[2019-11-18 09:29:58,455 INFO] Loading train dataset from ../bert_data/cnndm.train.111.bert.pt, number of examples: 2000\n",
      "[2019-11-18 09:30:08,540 INFO] Step 2400/ 5000; xent: 2.96; lr: 0.0000048;  30 docs/s;   1628 sec\n",
      "[2019-11-18 09:30:42,253 INFO] Step 2450/ 5000; xent: 2.96; lr: 0.0000049;  30 docs/s;   1662 sec\n",
      "[2019-11-18 09:31:15,927 INFO] Step 2500/ 5000; xent: 3.01; lr: 0.0000050;  30 docs/s;   1696 sec\n",
      "[2019-11-18 09:31:49,570 INFO] Step 2550/ 5000; xent: 3.02; lr: 0.0000051;  30 docs/s;   1729 sec\n",
      "[2019-11-18 09:32:12,626 INFO] Loading train dataset from ../bert_data/cnndm.train.49.bert.pt, number of examples: 2001\n",
      "[2019-11-18 09:32:23,505 INFO] Step 2600/ 5000; xent: 3.03; lr: 0.0000052;  30 docs/s;   1763 sec\n",
      "[2019-11-18 09:32:57,344 INFO] Step 2650/ 5000; xent: 3.00; lr: 0.0000053;  30 docs/s;   1797 sec\n",
      "[2019-11-18 09:33:30,965 INFO] Step 2700/ 5000; xent: 3.05; lr: 0.0000054;  30 docs/s;   1831 sec\n",
      "[2019-11-18 09:34:04,529 INFO] Step 2750/ 5000; xent: 3.01; lr: 0.0000055;  30 docs/s;   1864 sec\n",
      "[2019-11-18 09:34:26,908 INFO] Loading train dataset from ../bert_data/cnndm.train.59.bert.pt, number of examples: 2000\n",
      "[2019-11-18 09:34:38,418 INFO] Step 2800/ 5000; xent: 2.99; lr: 0.0000056;  30 docs/s;   1898 sec\n",
      "[2019-11-18 09:35:12,195 INFO] Step 2850/ 5000; xent: 2.94; lr: 0.0000057;  30 docs/s;   1932 sec\n",
      "[2019-11-18 09:35:45,671 INFO] Step 2900/ 5000; xent: 3.03; lr: 0.0000058;  30 docs/s;   1966 sec\n",
      "[2019-11-18 09:36:19,444 INFO] Step 2950/ 5000; xent: 2.94; lr: 0.0000059;  30 docs/s;   1999 sec\n",
      "[2019-11-18 09:36:40,381 INFO] Loading train dataset from ../bert_data/cnndm.train.84.bert.pt, number of examples: 2000\n",
      "[2019-11-18 09:36:53,308 INFO] Step 3000/ 5000; xent: 2.94; lr: 0.0000060;  30 docs/s;   2033 sec\n",
      "[2019-11-18 09:36:53,309 INFO] Saving checkpoint ../models/bert_classifier/model_step_3000.pt\n",
      "[2019-11-18 09:37:31,552 INFO] Step 3050/ 5000; xent: 3.04; lr: 0.0000061;  26 docs/s;   2071 sec\n",
      "[2019-11-18 09:38:05,146 INFO] Step 3100/ 5000; xent: 2.98; lr: 0.0000062;  30 docs/s;   2105 sec\n",
      "[2019-11-18 09:38:38,923 INFO] Step 3150/ 5000; xent: 2.97; lr: 0.0000063;  30 docs/s;   2139 sec\n",
      "[2019-11-18 09:38:59,481 INFO] Loading train dataset from ../bert_data/cnndm.train.137.bert.pt, number of examples: 2000\n",
      "[2019-11-18 09:39:13,076 INFO] Step 3200/ 5000; xent: 2.95; lr: 0.0000064;  30 docs/s;   2173 sec\n",
      "[2019-11-18 09:39:46,735 INFO] Step 3250/ 5000; xent: 2.95; lr: 0.0000065;  30 docs/s;   2207 sec\n",
      "[2019-11-18 09:40:20,672 INFO] Step 3300/ 5000; xent: 3.02; lr: 0.0000066;  30 docs/s;   2241 sec\n",
      "[2019-11-18 09:40:54,099 INFO] Step 3350/ 5000; xent: 2.91; lr: 0.0000067;  30 docs/s;   2274 sec\n",
      "[2019-11-18 09:41:13,715 INFO] Loading train dataset from ../bert_data/cnndm.train.13.bert.pt, number of examples: 2001\n",
      "[2019-11-18 09:41:27,783 INFO] Step 3400/ 5000; xent: 2.99; lr: 0.0000068;  30 docs/s;   2308 sec\n",
      "[2019-11-18 09:42:01,509 INFO] Step 3450/ 5000; xent: 2.94; lr: 0.0000069;  30 docs/s;   2341 sec\n",
      "[2019-11-18 09:42:35,061 INFO] Step 3500/ 5000; xent: 2.99; lr: 0.0000070;  30 docs/s;   2375 sec\n",
      "[2019-11-18 09:43:08,669 INFO] Step 3550/ 5000; xent: 2.91; lr: 0.0000071;  30 docs/s;   2409 sec\n",
      "[2019-11-18 09:43:27,708 INFO] Loading train dataset from ../bert_data/cnndm.train.135.bert.pt, number of examples: 1999\n",
      "[2019-11-18 09:43:42,686 INFO] Step 3600/ 5000; xent: 2.82; lr: 0.0000072;  30 docs/s;   2443 sec\n",
      "[2019-11-18 09:44:16,334 INFO] Step 3650/ 5000; xent: 2.98; lr: 0.0000073;  30 docs/s;   2476 sec\n",
      "[2019-11-18 09:44:50,055 INFO] Step 3700/ 5000; xent: 2.88; lr: 0.0000074;  30 docs/s;   2510 sec\n",
      "[2019-11-18 09:45:23,784 INFO] Step 3750/ 5000; xent: 2.84; lr: 0.0000075;  30 docs/s;   2544 sec\n",
      "[2019-11-18 09:45:41,429 INFO] Loading train dataset from ../bert_data/cnndm.train.71.bert.pt, number of examples: 1999\n",
      "[2019-11-18 09:45:57,608 INFO] Step 3800/ 5000; xent: 2.89; lr: 0.0000076;  30 docs/s;   2577 sec\n",
      "[2019-11-18 09:46:31,217 INFO] Step 3850/ 5000; xent: 2.90; lr: 0.0000077;  30 docs/s;   2611 sec\n",
      "[2019-11-18 09:47:04,596 INFO] Step 3900/ 5000; xent: 2.94; lr: 0.0000078;  30 docs/s;   2644 sec\n",
      "[2019-11-18 09:47:38,097 INFO] Step 3950/ 5000; xent: 2.97; lr: 0.0000079;  30 docs/s;   2678 sec\n",
      "[2019-11-18 09:47:55,874 INFO] Loading train dataset from ../bert_data/cnndm.train.9.bert.pt, number of examples: 1999\n",
      "[2019-11-18 09:48:12,040 INFO] Step 4000/ 5000; xent: 2.97; lr: 0.0000080;  30 docs/s;   2712 sec\n",
      "[2019-11-18 09:48:12,042 INFO] Saving checkpoint ../models/bert_classifier/model_step_4000.pt\n",
      "[2019-11-18 09:48:50,045 INFO] Step 4050/ 5000; xent: 2.89; lr: 0.0000081;  27 docs/s;   2750 sec\n",
      "[2019-11-18 09:49:23,594 INFO] Step 4100/ 5000; xent: 2.94; lr: 0.0000082;  30 docs/s;   2783 sec\n",
      "[2019-11-18 09:49:57,475 INFO] Step 4150/ 5000; xent: 2.78; lr: 0.0000083;  30 docs/s;   2817 sec\n",
      "[2019-11-18 09:50:13,711 INFO] Loading train dataset from ../bert_data/cnndm.train.105.bert.pt, number of examples: 2001\n",
      "[2019-11-18 09:50:31,330 INFO] Step 4200/ 5000; xent: 2.86; lr: 0.0000084;  30 docs/s;   2851 sec\n",
      "[2019-11-18 09:51:05,283 INFO] Step 4250/ 5000; xent: 2.93; lr: 0.0000085;  30 docs/s;   2885 sec\n",
      "[2019-11-18 09:51:38,839 INFO] Step 4300/ 5000; xent: 2.75; lr: 0.0000086;  30 docs/s;   2919 sec\n",
      "[2019-11-18 09:52:12,399 INFO] Step 4350/ 5000; xent: 2.96; lr: 0.0000087;  30 docs/s;   2952 sec\n",
      "[2019-11-18 09:52:27,523 INFO] Loading train dataset from ../bert_data/cnndm.train.50.bert.pt, number of examples: 2001\n",
      "[2019-11-18 09:52:46,518 INFO] Step 4400/ 5000; xent: 2.88; lr: 0.0000088;  30 docs/s;   2986 sec\n",
      "[2019-11-18 09:53:20,273 INFO] Step 4450/ 5000; xent: 2.86; lr: 0.0000089;  30 docs/s;   3020 sec\n",
      "[2019-11-18 09:53:53,990 INFO] Step 4500/ 5000; xent: 2.88; lr: 0.0000090;  30 docs/s;   3054 sec\n",
      "[2019-11-18 09:54:27,517 INFO] Step 4550/ 5000; xent: 2.87; lr: 0.0000091;  30 docs/s;   3087 sec\n",
      "[2019-11-18 09:54:41,985 INFO] Loading train dataset from ../bert_data/cnndm.train.106.bert.pt, number of examples: 1999\n",
      "[2019-11-18 09:55:01,472 INFO] Step 4600/ 5000; xent: 2.84; lr: 0.0000092;  30 docs/s;   3121 sec\n",
      "[2019-11-18 09:55:35,249 INFO] Step 4650/ 5000; xent: 2.90; lr: 0.0000093;  30 docs/s;   3155 sec\n",
      "[2019-11-18 09:56:09,047 INFO] Step 4700/ 5000; xent: 2.90; lr: 0.0000094;  30 docs/s;   3189 sec\n",
      "[2019-11-18 09:56:42,856 INFO] Step 4750/ 5000; xent: 2.87; lr: 0.0000095;  30 docs/s;   3223 sec\n",
      "[2019-11-18 09:56:55,777 INFO] Loading train dataset from ../bert_data/cnndm.train.38.bert.pt, number of examples: 2001\n",
      "[2019-11-18 09:57:16,873 INFO] Step 4800/ 5000; xent: 2.85; lr: 0.0000096;  30 docs/s;   3257 sec\n",
      "[2019-11-18 09:57:50,176 INFO] Step 4850/ 5000; xent: 2.84; lr: 0.0000097;  30 docs/s;   3290 sec\n",
      "[2019-11-18 09:58:23,902 INFO] Step 4900/ 5000; xent: 2.88; lr: 0.0000098;  30 docs/s;   3324 sec\n",
      "[2019-11-18 09:58:57,578 INFO] Step 4950/ 5000; xent: 2.83; lr: 0.0000099;  30 docs/s;   3357 sec\n",
      "[2019-11-18 09:59:10,063 INFO] Loading train dataset from ../bert_data/cnndm.train.24.bert.pt, number of examples: 2001\n",
      "[2019-11-18 09:59:31,590 INFO] Step 5000/ 5000; xent: 2.76; lr: 0.0000100;  29 docs/s;   3391 sec\n",
      "[2019-11-18 09:59:31,592 INFO] Saving checkpoint ../models/bert_classifier/model_step_5000.pt\n",
      "[2019-11-18 09:59:36,477 INFO] Loading train dataset from ../bert_data/cnndm.train.4.bert.pt, number of examples: 2001\n"
     ]
    }
   ],
   "source": [
    "!python train.py -mode train -encoder classifier -dropout 0.1 -bert_data_path ../bert_data/cnndm -model_path ../models/bert_classifier -lr 2e-3 -visible_gpus 0  -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 5000 -accum_count 2 -log_file ../logs/bert_classifier -use_interval true -warmup_steps 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестирование на валидационных и тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TRHkfCKHe0d7",
    "outputId": "9f086951-7ec0-497c-c4ed-26a78fb902af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-11-18 10:13:02,037 INFO] Loading checkpoint from ../models/bert_classifier/model_step_1000.pt\n",
      "Namespace(accum_count=1, batch_size=30000, bert_config_path='../bert_config_uncased_base.json', bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dataset='', decay_method='', dropout=0.1, encoder='classifier', ff_size=512, gpu_ranks=[0], heads=4, hidden_size=128, inter_layers=2, log_file='../logs/bert_classifier_valid', lr=1, max_grad_norm=0, mode='validate', model_path='../models/bert_classifier', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../results/cnndm', rnn_size=512, save_checkpoint_steps=5, seed=666, temp_dir='../temp', test_all=True, test_from='', train_from='', train_steps=1000, use_interval=True, visible_gpus='0', warmup_steps=8000, world_size=1)\n",
      "[2019-11-18 10:13:15,578 INFO] Loading valid dataset from ../bert_data/cnndm.valid.0.bert.pt, number of examples: 2001\n",
      "gpu_rank 0\n",
      "[2019-11-18 10:13:15,580 INFO] * number of parameters: 109483009\n",
      "[2019-11-18 10:13:53,966 INFO] Loading valid dataset from ../bert_data/cnndm.valid.1.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:14:32,359 INFO] Loading valid dataset from ../bert_data/cnndm.valid.2.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:15:10,357 INFO] Loading valid dataset from ../bert_data/cnndm.valid.3.bert.pt, number of examples: 2000\n",
      "[2019-11-18 10:15:48,705 INFO] Loading valid dataset from ../bert_data/cnndm.valid.4.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:16:26,810 INFO] Loading valid dataset from ../bert_data/cnndm.valid.5.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:17:05,133 INFO] Loading valid dataset from ../bert_data/cnndm.valid.6.bert.pt, number of examples: 1362\n",
      "[2019-11-18 10:17:30,903 INFO] Validation xent: 6.64862 at step 1000\n",
      "[2019-11-18 10:17:30,973 INFO] Loading checkpoint from ../models/bert_classifier/model_step_2000.pt\n",
      "Namespace(accum_count=1, batch_size=30000, bert_config_path='../bert_config_uncased_base.json', bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dataset='', decay_method='', dropout=0.1, encoder='classifier', ff_size=512, gpu_ranks=[0], heads=4, hidden_size=128, inter_layers=2, log_file='../logs/bert_classifier_valid', lr=1, max_grad_norm=0, mode='validate', model_path='../models/bert_classifier', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../results/cnndm', rnn_size=512, save_checkpoint_steps=5, seed=666, temp_dir='../temp', test_all=True, test_from='', train_from='', train_steps=1000, use_interval=True, visible_gpus='0', warmup_steps=8000, world_size=1)\n",
      "[2019-11-18 10:17:40,408 INFO] Loading valid dataset from ../bert_data/cnndm.valid.0.bert.pt, number of examples: 2001\n",
      "gpu_rank 0\n",
      "[2019-11-18 10:17:40,412 INFO] * number of parameters: 109483009\n",
      "[2019-11-18 10:18:18,775 INFO] Loading valid dataset from ../bert_data/cnndm.valid.1.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:18:57,138 INFO] Loading valid dataset from ../bert_data/cnndm.valid.2.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:19:35,109 INFO] Loading valid dataset from ../bert_data/cnndm.valid.3.bert.pt, number of examples: 2000\n",
      "[2019-11-18 10:20:13,423 INFO] Loading valid dataset from ../bert_data/cnndm.valid.4.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:20:51,497 INFO] Loading valid dataset from ../bert_data/cnndm.valid.5.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:21:29,809 INFO] Loading valid dataset from ../bert_data/cnndm.valid.6.bert.pt, number of examples: 1362\n",
      "[2019-11-18 10:21:55,590 INFO] Validation xent: 6.10015 at step 2000\n",
      "[2019-11-18 10:21:55,612 INFO] Loading checkpoint from ../models/bert_classifier/model_step_3000.pt\n",
      "Namespace(accum_count=1, batch_size=30000, bert_config_path='../bert_config_uncased_base.json', bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dataset='', decay_method='', dropout=0.1, encoder='classifier', ff_size=512, gpu_ranks=[0], heads=4, hidden_size=128, inter_layers=2, log_file='../logs/bert_classifier_valid', lr=1, max_grad_norm=0, mode='validate', model_path='../models/bert_classifier', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../results/cnndm', rnn_size=512, save_checkpoint_steps=5, seed=666, temp_dir='../temp', test_all=True, test_from='', train_from='', train_steps=1000, use_interval=True, visible_gpus='0', warmup_steps=8000, world_size=1)\n",
      "[2019-11-18 10:22:05,081 INFO] Loading valid dataset from ../bert_data/cnndm.valid.0.bert.pt, number of examples: 2001\n",
      "gpu_rank 0\n",
      "[2019-11-18 10:22:05,084 INFO] * number of parameters: 109483009\n",
      "[2019-11-18 10:22:43,450 INFO] Loading valid dataset from ../bert_data/cnndm.valid.1.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:23:21,800 INFO] Loading valid dataset from ../bert_data/cnndm.valid.2.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:23:59,759 INFO] Loading valid dataset from ../bert_data/cnndm.valid.3.bert.pt, number of examples: 2000\n",
      "[2019-11-18 10:24:38,078 INFO] Loading valid dataset from ../bert_data/cnndm.valid.4.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:25:16,153 INFO] Loading valid dataset from ../bert_data/cnndm.valid.5.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:25:54,455 INFO] Loading valid dataset from ../bert_data/cnndm.valid.6.bert.pt, number of examples: 1362\n",
      "[2019-11-18 10:26:20,234 INFO] Validation xent: 5.9464 at step 3000\n",
      "[2019-11-18 10:26:20,259 INFO] Loading checkpoint from ../models/bert_classifier/model_step_4000.pt\n",
      "Namespace(accum_count=1, batch_size=30000, bert_config_path='../bert_config_uncased_base.json', bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dataset='', decay_method='', dropout=0.1, encoder='classifier', ff_size=512, gpu_ranks=[0], heads=4, hidden_size=128, inter_layers=2, log_file='../logs/bert_classifier_valid', lr=1, max_grad_norm=0, mode='validate', model_path='../models/bert_classifier', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../results/cnndm', rnn_size=512, save_checkpoint_steps=5, seed=666, temp_dir='../temp', test_all=True, test_from='', train_from='', train_steps=1000, use_interval=True, visible_gpus='0', warmup_steps=8000, world_size=1)\n",
      "[2019-11-18 10:26:29,882 INFO] Loading valid dataset from ../bert_data/cnndm.valid.0.bert.pt, number of examples: 2001\n",
      "gpu_rank 0\n",
      "[2019-11-18 10:26:29,884 INFO] * number of parameters: 109483009\n",
      "[2019-11-18 10:27:08,249 INFO] Loading valid dataset from ../bert_data/cnndm.valid.1.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:27:46,599 INFO] Loading valid dataset from ../bert_data/cnndm.valid.2.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:28:24,553 INFO] Loading valid dataset from ../bert_data/cnndm.valid.3.bert.pt, number of examples: 2000\n",
      "[2019-11-18 10:29:02,876 INFO] Loading valid dataset from ../bert_data/cnndm.valid.4.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:29:40,949 INFO] Loading valid dataset from ../bert_data/cnndm.valid.5.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:30:19,260 INFO] Loading valid dataset from ../bert_data/cnndm.valid.6.bert.pt, number of examples: 1362\n",
      "[2019-11-18 10:30:45,037 INFO] Validation xent: 5.90544 at step 4000\n",
      "[2019-11-18 10:30:45,073 INFO] Loading checkpoint from ../models/bert_classifier/model_step_5000.pt\n",
      "Namespace(accum_count=1, batch_size=30000, bert_config_path='../bert_config_uncased_base.json', bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dataset='', decay_method='', dropout=0.1, encoder='classifier', ff_size=512, gpu_ranks=[0], heads=4, hidden_size=128, inter_layers=2, log_file='../logs/bert_classifier_valid', lr=1, max_grad_norm=0, mode='validate', model_path='../models/bert_classifier', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../results/cnndm', rnn_size=512, save_checkpoint_steps=5, seed=666, temp_dir='../temp', test_all=True, test_from='', train_from='', train_steps=1000, use_interval=True, visible_gpus='0', warmup_steps=8000, world_size=1)\n",
      "[2019-11-18 10:30:54,543 INFO] Loading valid dataset from ../bert_data/cnndm.valid.0.bert.pt, number of examples: 2001\n",
      "gpu_rank 0\n",
      "[2019-11-18 10:30:54,545 INFO] * number of parameters: 109483009\n",
      "[2019-11-18 10:31:32,893 INFO] Loading valid dataset from ../bert_data/cnndm.valid.1.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:32:11,255 INFO] Loading valid dataset from ../bert_data/cnndm.valid.2.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:32:49,218 INFO] Loading valid dataset from ../bert_data/cnndm.valid.3.bert.pt, number of examples: 2000\n",
      "[2019-11-18 10:33:27,528 INFO] Loading valid dataset from ../bert_data/cnndm.valid.4.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:34:05,599 INFO] Loading valid dataset from ../bert_data/cnndm.valid.5.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:34:43,899 INFO] Loading valid dataset from ../bert_data/cnndm.valid.6.bert.pt, number of examples: 1362\n",
      "[2019-11-18 10:35:09,668 INFO] Validation xent: 5.69608 at step 5000\n",
      "[2019-11-18 10:35:09,692 INFO] PPL [(5.696077902868012, '../models/bert_classifier/model_step_5000.pt'), (5.905437232330224, '../models/bert_classifier/model_step_4000.pt'), (5.946399392779874, '../models/bert_classifier/model_step_3000.pt')]\n",
      "[2019-11-18 10:35:09,692 INFO] Loading checkpoint from ../models/bert_classifier/model_step_5000.pt\n",
      "Namespace(accum_count=1, batch_size=30000, bert_config_path='../bert_config_uncased_base.json', bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dataset='', decay_method='', dropout=0.1, encoder='classifier', ff_size=512, gpu_ranks=[0], heads=4, hidden_size=128, inter_layers=2, log_file='../logs/bert_classifier_valid', lr=1, max_grad_norm=0, mode='validate', model_path='../models/bert_classifier', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../results/cnndm', rnn_size=512, save_checkpoint_steps=5, seed=666, temp_dir='../temp', test_all=True, test_from='', train_from='', train_steps=1000, use_interval=True, visible_gpus='0', warmup_steps=8000, world_size=1)\n",
      "[2019-11-18 10:35:11,966 INFO] Loading test dataset from ../bert_data/cnndm.test.0.bert.pt, number of examples: 2001\n",
      "gpu_rank 0\n",
      "[2019-11-18 10:35:11,969 INFO] * number of parameters: 109483009\n",
      "[2019-11-18 10:35:50,013 INFO] Loading test dataset from ../bert_data/cnndm.test.1.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:36:28,586 INFO] Loading test dataset from ../bert_data/cnndm.test.2.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:37:06,837 INFO] Loading test dataset from ../bert_data/cnndm.test.3.bert.pt, number of examples: 2001\n",
      "[2019-11-18 10:37:45,285 INFO] Loading test dataset from ../bert_data/cnndm.test.4.bert.pt, number of examples: 2000\n",
      "[2019-11-18 10:38:23,497 INFO] Loading test dataset from ../bert_data/cnndm.test.5.bert.pt, number of examples: 1485\n",
      "11489\n",
      "11489\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 342, in <module>\n",
      "    wait_and_validate(args, device_id)\n",
      "  File \"train.py\", line 138, in wait_and_validate\n",
      "    test(args,  device_id, cp, step)\n",
      "  File \"train.py\", line 216, in test\n",
      "    trainer.test(test_iter,step)\n",
      "  File \"/content/BertSum/src/models/trainer.py\", line 297, in test\n",
      "    rouges = test_rouge(self.args.temp_dir, can_path, gold_path)\n",
      "  File \"/content/BertSum/src/others/utils.py\", line 79, in test_rouge\n",
      "    r = pyrouge.Rouge155(temp_dir=temp_dir)\n",
      "  File \"/content/BertSum/src/others/pyrouge.py\", line 123, in __init__\n",
      "    self.__set_rouge_dir(rouge_dir)\n",
      "  File \"/content/BertSum/src/others/pyrouge.py\", line 439, in __set_rouge_dir\n",
      "    self._home_dir = self.__get_rouge_home_dir_from_settings()\n",
      "  File \"/content/BertSum/src/others/pyrouge.py\", line 453, in __get_rouge_home_dir_from_settings\n",
      "    with open(self._settings_file) as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/root/.pyrouge/settings.ini'\n"
     ]
    }
   ],
   "source": [
    "!python train.py -mode validate -bert_data_path ../bert_data/cnndm -model_path ../models/bert_classifier  -visible_gpus 0  -gpu_ranks 0 -batch_size 30000  -log_file ../logs/bert_classifier_valid  -result_path ../results/cnndm -test_all -block_trigram true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5mVQNPzaHBDZ"
   },
   "outputs": [],
   "source": [
    "# extracted summary\n",
    "N = 20\n",
    "with open(\"/content/BertSum/results/cnndm_step5000.candidate\") as f:\n",
    "    dec = [next(f) for x in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONnGsk_6YC8T"
   },
   "outputs": [],
   "source": [
    "# target summary\n",
    "N = 20\n",
    "with open(\"/content/BertSum/results/cnndm_step5000.gold\") as f:\n",
    "    ref = [next(f) for x in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "sTUMy-7lTROZ",
    "outputId": "bbe0ae3c-4cf3-4512-89b5-011b888a9c6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the 79th masters tournament gets underway at augusta national on thursday',\n",
       " 'rory mcilroy and tiger woods will be the star attractions in the field bidding for the green jacket at 2015 masters',\n",
       " 'mcilroy , justin rose , ian poulter , graeme mcdowell and more gave sportsmail the verdict on each hole at augusta',\n",
       " 'click on the brilliant interactive graphic below for details on each hole of the masters 2015 course',\n",
       " 'click here for all the latest news from the masters 2015\\n']"
      ]
     },
     "execution_count": 111,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref[0].split('<q>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "vs-rkv00TNma",
    "outputId": "344aa303-e903-4a65-d5a3-cee14a889429"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to help get you in the mood for the first major of the year , rory mcilroy , ian poulter , graeme mcdowell and justin rose , plus past masters champions nick faldo and charl schwartzel , give the lowdown on every hole at the world-famous augusta national golf club .',\n",
       " 'the masters 2015 is almost here .',\n",
       " 'click on the graphic below to get a closer look at what the biggest names in the game will face when they tee off on thursday .\\n']"
      ]
     },
     "execution_count": 109,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec[0].split('<q>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "QblpOaQIYdII",
    "outputId": "5b8a0520-1b19-4c61-e717-d3c3998e632d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"jeff powell looks ahead to saturday 's fight at the mgm grand\",\n",
       " 'floyd mayweather takes on manny pacquiao in $ 300m showdown',\n",
       " 'both fighters arrived in las vegas on tuesday with public appearances',\n",
       " 'read : mayweather makes official arrival ahead of manny pacquiao fight',\n",
       " 'al haymon : the man behind mayweather who is revolutionising boxing',\n",
       " \"mayweather vs pacquiao takes centre stage ... but who 's on the undercard ?\\n\"]"
      ]
     },
     "execution_count": 113,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref[1].split('<q>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "-FpWNGk_U4E8",
    "outputId": "664943ad-edd6-415e-8c8f-79eceb7e4a28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"sportsmail 's boxing correspondent jeff powell looks ahead to saturday 's mega-fight at the mgm grand after witnessing floyd mayweather and manny pacquiao 's grand arrivals in las vegas .\",\n",
       " 'both boxers made public appearances on tuesday as their $ 300million showdown draws ever closer , and our man powell was there .',\n",
       " \"powell reflects on the pair 's arrivals on the las vegas strip and looks forward to the rest of the week .\\n\"]"
      ]
     },
     "execution_count": 112,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec[1].split('<q>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "9ZCaWmqEYgLN",
    "outputId": "c0381a73-6225-4669-f022-b3043f493651"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gary locke has been interim manager since start of february',\n",
       " 'locke has won two and drawn four of his seven games in charge',\n",
       " 'the 37-year-old took over when allan johnston quit\\n']"
      ]
     },
     "execution_count": 114,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref[2].split('<q>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "jMiC_NqjYpQS",
    "outputId": "75ab376b-1744-4182-d756-1370496e6b3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kilmarnock interim manager gary locke has been given the role on a permanent basis after signing a three-year deal .',\n",
       " 'the former hearts boss joined the club as assistant boss to allan johnston last summer but took control of the team when his ex-tynecastle team-mate quit at the start of february .',\n",
       " 'the 39-year-old - who will speak at a press conference on friday morning - has lost just once in seven games since taking over at rugby park .\\n']"
      ]
     },
     "execution_count": 115,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec[2].split('<q>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mSkCIBD_YsaR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sem_11_summarization.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
